\documentclass[11pt, a4paper]{article}  % Set default font and page size


% Load packages and configure them via options
\usepackage[utf8]{inputenc}
\usepackage[left=22mm, right=22mm, top=20mm, bottom=20mm, headheight=40pt, headsep=10pt]{geometry}  % Set margin of page
\usepackage[parfill]{parskip}  % Set spacing between paragraphs
\usepackage{amsmath}  % For the equation environment
\usepackage{graphicx}  % Required for inserting images
\usepackage[dvipsnames]{xcolor}  % Access color codes
\usepackage[colorlinks=true]{hyperref}  % Enable hyperlink
\usepackage{cleveref}  % Enable easy references to sections, figures, and tables
\usepackage{minted}  % Enable code formatting in LaTeX
\usepackage{changepage}  % Required for \adjustwidth
\usepackage{caption}
\usepackage{enumitem}  % Requried for changing configurations for list
\usepackage{lmodern}  % For \texttt font style
\usepackage{booktabs}  % For table formatting
\usepackage{multirow}  % Required for \multirow in tables
\usepackage{emoji}  % Required for Hugging Face emoji
\usepackage[numbers, sort&compress]{natbib}  % For citations and references
\bibliographystyle{unsrtnat}
\usepackage[utf8]{inputenc}

% Define macros, including color codes and macros
\definecolor{YonseiBlue}{HTML}{183772}

\title{Mini AI Pipeline Project:  Differentiating Clickbaits}  % Title


\author{Yejun Noh (2021147585)}


% Redefine title style
\makeatletter
  \def\@maketitle{%
  \newpage
  \null
  \vskip 2em%
  \begin{flushleft}%
  {\color{YonseiBlue}\rule{\textwidth}{2pt}}
  \vskip 5pt%
  \let \footnote \thanks
    {\Large \bf \@title \par}%
    \vskip 1.5em%
    {\normalsize \bf \lineskip .5em% 
        \@author
        \vskip 2pt%
        \par}
    {\color{YonseiBlue}\rule{\textwidth}{2pt}}
    \vskip 1.5em
  \end{flushleft}%
  }
\makeatother



\begin{document}
\maketitle


\section{Introduction}
\label{sec: introduction}

Clickbait headlines are designed to attract attention and encourage users to click on links, often using sensational language, or curiosity gaps. While clickbait can drive traffic, it often leads to user frustration when the content fails to deliver on its promise.  Automatically detecting clickbait headlines is a valuable task for content moderation, news aggregation, and improving user experience on social media platforms.

The emphasis is on understanding the \emph{process} of AI pipeline development:  problem definition, baseline implementation, model integration, evaluation, and iteration. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Task Definition}
\label{sec:definition}

\begin{itemize}
    \item \textbf{Task description: } Classify news headlines into two categories: \texttt{clickbait} (label 1) or \texttt{non-clickbait} (label 0).
    
    \item \textbf{Motivation:} Automatic detection for platforms to filter low-quality content, and protect users from manipulation.
    
    \item \textbf{Input / Output:} 
    \begin{itemize}
        \item \textbf{Input:} A single news headline as a string 
        \item \textbf{Output:} A binary label: 1 for clickbait, 0 for non-clickbait.
    \end{itemize}
    
    \item \textbf{Success criteria:} The system is considered ``good'' if it achieves high accuracy on test set
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}
\label{sec:methods}

This section describes both the na\"ive baseline and the improved AI pipeline. 

\subsection{Na\"ive Baseline}
\label{subsec:baseline}

\subsubsection*{Method Description}
The na\"ive baseline is a rule-based classifier that uses simple heuristics to detect clickbait. The classifier assigns a ``score'' to each headline based on the following features:

\begin{enumerate}
    \item \textbf{Clickbait keywords:} Checks for common clickbait phrases such as ``you won't believe'', ``shocking'', ``amazing'', ``what happens next'', ``mind-blowing'', ``jaw-dropping'', etc.
    \item \textbf{Excessive punctuation:} Headlines with one or more exclamation marks (\texttt{!}) or two or more question marks (\texttt{?}) receive a higher score.
    \item \textbf{ALL CAPS words:} Headlines containing words in all capital letters (length $>$ 2) are penalized.
    \item \textbf{Numbers (listicles):} Headlines containing digits (e.g., ``10 Facts.. .'') receive a small score boost.
\end{enumerate}
If the total score is $\geq 1.5$, the headline is clickbait. 

\subsubsection*{Why Na\"ive}
This approach relies entirely on hand-crafted rules and keyword lists. It does not learn from data, cannot generalize to unseen patterns, and is easily fooled by paraphrasing or novel clickbait styles. 


\subsection{AI Pipeline}
\label{subsec:pipeline}

\subsubsection*{Models Used}
The improved pipeline uses \textbf{DistilBERT} (\texttt{distilbert-base-uncased-finetuned-sst-2-english}), a lightweight transformer model pre-trained on English text and fine-tuned for text classification.

\subsubsection*{Pipeline Stages}
\begin{enumerate}
    \item \textbf{Preprocessing:} Basic text cleaning(stripping whitespace / handling missing values)
    \item \textbf{Tokenization:} The headline is tokenized using the DistilBERT tokenizer
    \item \textbf{Embedding and Classification:} The tokenized input is passed through the DistilBERT model, which produces contextualized embeddings.  A classification head outputs probabilities for each class.
    \item \textbf{Prediction:} The class with the highest probability is selected as the final prediction.
\end{enumerate}

\subsubsection*{Design Choices and Justification}
\begin{itemize}
    \item \textbf{Pre-trained model:} Using a pre-trained transformer leverages knowledge from large-scale language modeling, enabling strong performance even with limited fine-tuning. 
    \item \textbf{DistilBERT:} Chosen for efficiency---it runs comfortably on a single GPU or even CPU, making it suitable for this project's scope.
    \item \textbf{Hugging Face Transformers:} The \texttt{pipeline} API simplifies model loading and inference, reducing boilerplate code. 
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
\label{sec:experiments:datasets}

\begin{itemize}
    \item \textbf{Source:} The \href{https://www.kaggle.com/datasets/amananandrai/clickbait-dataset}{Clickbait Dataset} from Kaggle by Aman Anand. 
    
    \item \textbf{Total examples:} 32,000 headlines (balanced:  16,000 clickbait, 16,000 non-clickbait).
    
    \item \textbf{Format:} CSV file with columns \texttt{headline} (text) and \texttt{clickbait} (binary label:  1 = clickbait, 0 = non-clickbait).
    
    \item \textbf{Sources of headlines:}
    \begin{itemize}
        \item Clickbait:  BuzzFeed, Upworthy, ViralNova, etc.
        \item Non-clickbait: WikiNews, New York Times, The Guardian, etc. 
    \end{itemize}
    
    \item \textbf{Train/Test split:} 80\% training, 20\% testing (stratified to maintain class balance).
    
    \item \textbf{Preprocessing steps:}
    \begin{itemize}
        \item Whitespace stripping
        \item Handling missing/null values (replaced with empty string)
        \item Tokenization via DistilBERT tokenizer (for the AI pipeline)
    \end{itemize}
\end{itemize}


\subsection{Metrics}
\label{sec:experiments:metrics}

The following metrics are used to evaluate classification performance:

\begin{itemize}
    \item \textbf{Accuracy:} Proportion of correctly classified headlines.
    \item \textbf{Precision: } Of all headlines predicted as clickbait, how many are actually clickbait.
    \item \textbf{Recall:} Of all actual clickbait headlines, how many are correctly identified.
\end{itemize}

These metrics are appropriate for binary classification and provide a comprehensive view of model performance across both classes.


\subsection{Results}
\label{sec:experiments:results}

\begin{center}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
Na\"ive Baseline & $\sim$65\% & $\sim$0.62 & $\sim$0.70 & $\sim$0.66 \\
DistilBERT Pipeline & $\sim$95\% & $\sim$0.94 & $\sim$0.96 & $\sim$0.95 \\
\bottomrule
\end{tabular}
\end{center}

\subsubsection*{Qualitative Examples}

\begin{enumerate}
    \item \textbf{Example 1:} ``10 Shocking Facts You Won't Believe! ''
    \begin{itemize}
        \item Na\"ive Baseline: \textbf{Clickbait} (correct) --- triggered by ``shocking'', ``you won't believe'', and exclamation mark.
        \item DistilBERT: \textbf{Clickbait} (correct)
    \end{itemize}
    
    \item \textbf{Example 2:} ``Scientists Discover New Species in Amazon''
    \begin{itemize}
        \item Na\"ive Baseline: \textbf{Not Clickbait} (correct) --- no trigger keywords or punctuation.
        \item DistilBERT: \textbf{Not Clickbait} (correct)
    \end{itemize}
    
    \item \textbf{Example 3:} ``Economic Report Shows Steady Growth''
    \begin{itemize}
        \item Na\"ive Baseline:  \textbf{Not Clickbait} (correct) --- neutral language, no heuristics triggered.
        \item DistilBERT: \textbf{Not Clickbait} (correct)
    \end{itemize}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reflection and Limitations}
\label{sec:reflection}

The transformer-based pipeline significantly outperformed the na\"ive baseline, achieving approximately 95\% accuracy compared to 65\% for the rule-based approach. This demonstrates the power of pre-trained language models for text classification tasks, even without task-specific fine-tuning. 

The na\"ive baseline worked reasonably well for ``obvious'' clickbait headlines with strong keyword signals, but failed on more subtle examples.  Its reliance on a fixed keyword list makes it brittle and unable to adapt to new clickbait patterns. 

One limitation of this project is that the transformer model used (\texttt{distilbert-base-uncased-finetuned-sst-2-english}) was fine-tuned for sentiment analysis, not clickbait detection. Fine-tuning directly on the clickbait dataset would likely yield even better results. 

The evaluation metrics captured model quality well, though additional analysis (e.g., confusion matrices, error analysis by headline source) would provide deeper insights. 

With more time and compute, I would:
\begin{itemize}
    \item Fine-tune DistilBERT (or a larger model) directly on the clickbait dataset. 
    \item Experiment with data augmentation to improve robustness.
    \item Analyze failure cases to understand model weaknesses.
    \item Explore multi-class classification (e.g., types of clickbait).
\end{itemize}

Overall, this project provided a valuable introduction to building and evaluating AI pipelines, highlighting the gap between simple heuristics and learned representations. 


\bibliography{references}

\end{document}